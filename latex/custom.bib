% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{wu2016google,
  title   = {Google's neural machine translation system: Bridging the gap between human and machine translation},
  author  = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal = {arXiv preprint arXiv:1609.08144},
  year    = {2016}
}
@inproceedings{He-2019-effective,
  author    = {He, Han
               and Wu, Lei
               and Yan, Hua
               and Gao, Zhimin
               and Feng, Yi
               and Townsend, George},
  editor    = {Satapathy, Suresh Chandra
               and Bhateja, Vikrant
               and Das, Swagatam},
  title     = {Effective Neural Solution for Multi-criteria Word Segmentation},
  booktitle = {Smart Intelligent Computing and Applications },
  year      = {2019},
  publisher = {Springer Singapore},
  address   = {Singapore},
  pages     = {133--142},
  abstract  = {We present a novel and elegant deep learning solution to train a single joint model on multi-criteria corpora for Chinese Word Segmentation (CWS) challenge. Our innovative design requires no private layers in model architecture, instead, introduces two artificial tokens at the beginning and ending of input sentence to specify the required target criteria. The rest of the model including Long Short-Term Memory (LSTM) layer and Conditional Random Fields (CRFs) layer remains unchanged and is shared across all datasets, keeping the size of parameter collection minimal and constant. On Bakeoff 2005 and Bakeoff 2008 datasets, our innovative design has surpassed the previous multi-criteria learning results. Testing results on two out of four datasets even have surpassed the latest state-of-the-art single-criterion learning scores. To the best knowledge, our design is the first one that has achieved the latest state-of-the-art performance on such large-scale datasets. Source codes and corpora of this paper are available on GitHub (https://github.com/hankcs/multi-criteria-cws).},
  isbn      = {978-981-13-1927-3}
}
@article{Gong-2019-switch,
  title        = {Switch-LSTMs for Multi-Criteria Chinese Word Segmentation},
  volume       = {33},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4610},
  doi          = {10.1609/aaai.v33i01.33016457},
  abstractnote = {&lt;p&gt;Multi-criteria Chinese word segmentation is a promising but challenging task, which exploits several different segmentation criteria and mines their common underlying knowledge. In this paper, we propose a flexible multi-criteria learning for Chinese word segmentation. Usually, a segmentation criterion could be decomposed into multiple sub-criteria, which are shareable with other segmentation criteria. The process of word segmentation is a routing among these sub-criteria. From this perspective, we present Switch-LSTMs to segment words, which consist of several long short-term memory neural networks (LSTM), and a switcher to automatically switch the routing among these LSTMs. With these auto-switched LSTMs, our model provides a more flexible solution for multi-criteria CWS, which is also easy to transfer the learned knowledge to new criteria. Experiments show that our model obtains significant improvements on eight corpora with heterogeneous segmentation criteria, compared to the previous method and single-criterion learning.&lt;/p&gt;},
  number       = {01},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Gong, Jingjing and Chen, Xinchi and Gui, Tao and Qiu, Xipeng},
  year         = {2019},
  month        = {Jul.},
  pages        = {6457-6464}
}
@article{ke2020unified,
  title   = {Unified multi-criteria chinese word segmentation with bert},
  author  = {Ke, Zhen and Shi, Liang and Meng, Erli and Wang, Bin and Qiu, Xipeng and Huang, Xuanjing},
  journal = {arXiv preprint arXiv:2004.05808},
  year    = {2020}
}
@article{Schuster-1997-bidirectional,
  author  = {Schuster, M. and Paliwal, K.K.},
  journal = {IEEE Transactions on Signal Processing},
  title   = {Bidirectional recurrent neural networks},
  year    = {1997},
  volume  = {45},
  number  = {11},
  pages   = {2673-2681},
  doi     = {10.1109/78.650093}
}
@article{graves-2005-framewise,
  title    = {Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  journal  = {Neural Networks},
  volume   = {18},
  number   = {5},
  pages    = {602-610},
  year     = {2005},
  note     = {IJCNN 2005},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/j.neunet.2005.06.042},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608005001206},
  author   = {Alex Graves and Jürgen Schmidhuber},
  abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.11An abbreviated version of some portions of this article appeared in (Graves and Schmidhuber, 2005), as part of the IJCNN 2005 conference proceedings, published under the IEEE copyright.}
}
@inproceedings{vaswani-2017-attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}
@inproceedings{goodfellow-2014-gan,
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Generative Adversarial Nets},
  url       = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
  volume    = {27},
  year      = {2014}
}
@article{xue-2015-ctb,
  title     = {The Penn Chinese TreeBank: Phrase structure annotation of a large corpus},
  volume    = {11},
  doi       = {10.1017/S135132490400364X},
  number    = {2},
  journal   = {Natural Language Engineering},
  publisher = {Cambridge University Press},
  author    = {XUE, NAIWEN and XIA, FEI and CHIOU, FU-DONG and PALMER, MARTA},
  year      = {2005},
  pages     = {207–238}
}
@article{wolf2019huggingface,
  title   = {Huggingface's transformers: State-of-the-art natural language processing},
  author  = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal = {arXiv preprint arXiv:1910.03771},
  year    = {2019}
}
@inproceedings{paszke-etal-2019-pytorch,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  url       = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
  volume    = {32},
  year      = {2019}
}
@inproceedings{loshchilov2018decoupled,
  title     = {Decoupled Weight Decay Regularization},
  author    = {Ilya Loshchilov and Frank Hutter},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=Bkg6RiCqY7}
}
@inproceedings{qiu2016overview,
  title     = {Overview of the {NLPCC-ICCPOL} 2016 Shared Task: Chinese Word Segmentation for Micro-blog Texts},
  author    = {Xipeng Qiu and Peng Qian and Zhan Shi},
  booktitle = {Proceedings of The Fifth Conference on Natural Language Processing and Chinese Computing \& The Twenty Fourth International Conference on Computer Processing of Oriental Languages},
  pages     = {901--906},
  year      = {2016}
}
@article{srivastava2014dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@inproceedings{zhang2021dilated,
  author    = {Zhang, Taozheng and Liao, Ziyao},
  title     = {Dilated Convolutional Neural Network with Joint Training for Chinese Word Segmentation},
  year      = {2021},
  isbn      = {9781450389464},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3460569.3460582},
  doi       = {10.1145/3460569.3460582},
  abstract  = {The task of Chinese word segmentation (CWS) is to segment a continuous Chinese text sequence into individual word sequences according to certain rules. As the most basic step of Chinese natural language processing tasks, the study of CWS is of great significance. In this paper, based on the method of deep learning, the radical features were added when pretraining the character vectors, which has led to the improvement of the outcome. Dilated Convolutional Neural Network (DCNN) was trained and compared with the baseline Bi-LSTM to verify the segmentation ability, and Conditional Random Field (CRF) was applied with two neural network models above-mentioned respectively to confirm the best model. The experiment results show that the DCNN model is better than the widely used Bi-LSTM model in the performance. The joint training method was also applied to further improve the training outcome, and it comes out the improvement on F1 value of joint training with several corpora.},
  booktitle = {2021 6th International Conference on Mathematics and Artificial Intelligence},
  pages     = {114–121},
  numpages  = {8},
  keywords  = {DCNN, Neural networks, Chinese word segmentation, Joint training},
  location  = {Chengdu, China},
  series    = {ICMAI 2021}
}
